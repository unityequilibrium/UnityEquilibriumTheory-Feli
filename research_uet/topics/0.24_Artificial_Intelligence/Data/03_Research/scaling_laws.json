{
    "source": "Kaplan et al., Scaling Laws for Neural Language Models (2020)",
    "description": "Power-law scaling of Test Loss (L) vs Compute (C), Data (D), and Parameters (N).",
    "relationships": {
        "L_N": "L(N) ~ (N_c / N)^alpha_N",
        "L_D": "L(D) ~ (D_c / D)^alpha_D",
        "L_C": "L(C) ~ (C_c / C)^alpha_C"
    },
    "constants": {
        "alpha_N": 0.076,
        "alpha_D": 0.095,
        "alpha_C": 0.050,
        "N_c_parameters": 8.8e13,
        "D_c_tokens": 5.4e13,
        "C_c_petaflops_days": 3.1e8
    },
    "uet_hypothesis": "The Scaling Law is the Equation of State for Information Gas. Alpha corresponds to the compressibility factor (Kappa) of the semantic space."
}
